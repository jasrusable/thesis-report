\documentclass[11pt,a4paper]{report}

\renewcommand{\baselinestretch}{1.5}

\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage[backend=bibtex]{biblatex}

\graphicspath{{./images/}}
\addbibresource{sources.bib}

\title{Indoor positioning using the comparison of digital imagery to 3D models}
\date{2015}
\author{Jason David Russell}

\begin{document}

\maketitle
\thispagestyle{empty}

\pagenumbering{roman}
\setcounter{page}{1}

\newpage
\chapter*{Plagiarism Declaration}
	\addcontentsline{toc}{chapter}{Plagiarism Declaration}
	\begin{enumerate}
		\item
			I know that plagiarism means taking and using the ideas, writings, works or inventions of another as if they were one's own. I know that plagiarism not only includes verbatim copying, but also the extensive use of another person's ideas without proper acknowledgement (which includes the proper use of quotation marks). I know that plagiarism covers this sort of use of material found in textual sources and from the Internet.
		\item
			I acknowledge and understand that plagiarism is wrong.
		\item
			I understand that my research must be accurately referenced. I have followed the rules and conventions concerning referencing, citation and the use of quotations as set out in the Departmental Guide.
		\item
			This assignment is my own work, or my group's own unique group assignment.
			I acknowledge that copying someone else's assignment, or part of it, is wrong, and that submitting identical work to others constitutes a form of plagiarism.
		\item
			I have not allowed, nor will I in the future allow, anyone to copy my work with the intention of passing it off as their own work.
	\end{enumerate}
	Jason David Russell

\newpage
\chapter*{Acknowledgements}
	\addcontentsline{toc}{chapter}{Acknowledgements}
	I acknowledge everything.

\newpage
\chapter*{Abstract}
	\addcontentsline{toc}{chapter}{Abstract}
	This paper investigates the viability of a new indoor positioning system which is to make use of the comparison of digital imagery to 3D models. An example scenario would be a person wishing to know which room they are in within a building. A 3D model of the building exists. The person takes a few photographs of their surroundings, the photographs are then compared to the 3D model and the result is a location estimate presented to the person. The process involves comparing the photograph to the 3D model of the building. The main focus of this paper is on the comparison of the photograph to the 3D model. Future work would involve the investigating the process actually determining the position of the photographer, given that the comparison of the photograph to the 3D model is successful.

\newpage
\tableofcontents

\newpage
\listoffigures

\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\chapter{Introduction}
	\section{Subject of the Report}
		This report concerns the viability of an alternate indoor positioning method. The method involves comparing a photograph to a 3D interior model in order to determine the location of the person or object which took the photograph. 3D interior models are becoming more and more prominent. New technological advancements and techniques are making it cheaper, quicker and easier to generate interior 3D models. Combining the recent prominence of 3D interior models with the ubiquity of camera equipped smart phones and a new indoor positioning system presents itself. This system depends upon the matching of a photograph to a 3D interior model in order to locate the photographer. The primary focus of this paper is on the matching of a photograph to a 3D interior model.
	
	\section{Problem statement}
		The objectives of this report are therefore to do something.
		
	\section{Research questions}
		\begin{enumerate}
			\item How can a photograph be matched to a 3D model?
			\item How does the level of detail of the model affect the matching?
			\item How does lighting affect the matching?
			\item How does the texture of modelled objects affect the matching?
			\item How does the quality of model renders affect the matching?
			\item How does the quality of the photograph affect the matching?
			\item What is the affect of image enhancements on the matching?
		\end{enumerate}
	
	\section{Plan of Development}
		This report begins by looking at work related to indoor positioning systems, including existing principles and techniques. It appears that there is very little work specifically related to comparing photographs to 3D models or to rendered images of 3D models. Various radio and non-radio indoor positioning technologies are covered in the related work chapter.
		
		After the related work chapter, the method is explained. The method contains three sections. The first section deals with the photograph, the second section concerns the 3D interior model and covers how a 3D model is created and modelled and rendered. The last section concerns the comparison of the photograph to the 3D model.
		
		After the method chapter, results are presented and discussed. Again, there are three sections in the results chapter, photograph, 3D model and comparison of the photograph to the model.

\newpage
\chapter{Related Work}
	No previous work pertaining specifically to the comparison of photography to 3D models has been found. As such, the sections to follow will discuss work related to indoor positioning systems in general. This will include various principles as well as applications. Two main branches of technologies will be covered, namely radio and non-radio technologies.
	
	\section{Radio technologies}
		Any wireless technology can in theory, be used for positioning. This is due to the nature of the propagation of electromagnetic waves through a medium. There are three main principles which make use of radio technologies, each of these principles will be outlined briefly in the subsections below. Some applications of these principles will also be discussed and presented.
	
	\subsection{Principles of radio technologies}
		\subsubsection{Time of arrival}
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{time_of_arrival}
				\caption{Time of arrival}
				\label{fig:time_of_arrival}
			\end{figure}
			
			Time of arrival (or time of flight) is the travel time of a radio signal from a single transmitter to a single remote receiver. The basic observable is time. A distance can be directly calculated using the known propagation velocity of signals with the basic observable of change in time. Location can then be determined using multi-lateration, this requires a setup of at least one receiver and three transmitters or vise versa.
			Required with such systems is the synchronization of clocks between receivers and transmitters in order to determine the propagation time of the signals.
			A significant pitfall of time of travel systems is that they suffer greatly from effects such as multi-path.
			\cite{k._pahlavan_wideband_1998}
			
			Figure \ref{fig:time_of_arrival} demonstrates the concept of time of arrival. The figure shows three transmitters with a single receiver in the centre of the transmitters. A radius is determined for each transmitter by calculating a distance using the known propagation velocity and measured change in time (received - transmitted). Tri-lateration is then used to determine a unique location of the receiver.
		
		\subsubsection{Received signal strength indication}
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{rssi}
				\caption{RSSI}
				\label{fig:rssi}
			\end{figure}
			Received signal strength indication (RSSI) is a measure of the power level received by a sensor. Distance can be approximated between a transmitter and receiver based on the relationship between the transmitted and received signal strength. Multi-lateration can be used to determine the location of a sensor.
			A downside to this technique is that the relationship between received signal strength and distance has to be predetermined.
			
			Figure \ref{fig:rssi} illustrates the principle of RSSI. Received signal levels decrease radially from the transmitter.
		
		\subsubsection{Angle of arrival}
			\begin{figure}[H]
				\centering
				\includegraphics[width=0.8\textwidth]{angle_of_arrival}
				\caption{Angle of arrival}
				\label{fig:angle_of_arrival}
			\end{figure}
			The principle of angle of arrival works be determining the angle from which a signal is received at a receiver. The angle of arrival is typically determined using the time difference of arrival between multiple antennas in a sensor array. Position is determined with multi-angulation using readings from multiple transmitters.
			Angle of arrival systems also suffer from effects such as multi-path.
			
			Figure \ref{fig:angle_of_arrival} illustrates the principle of angle of arrival. In the figure, $B_1$ and $B_2$ each represent a transmitter. Their signals are detected by the receiver, M.
			
	\subsection{Applications of radio technologies}
		\subsubsection{Wi-Fi based systems}
			Wi-Fi based technologies are based on measurements such as time of arrival (TOA), time difference of arrival (TDOA) and direction of arrival (DOA). However these techniques are severely impaired when line of sight is not achievable. 
			These techniques suffer due to objects such as walls and floors which attenuate and reflect the signal, directly affecting accuracies.
		
			An alternative Wi-Fi based geolocation method has been explored which makes use of relative signal strengths between transmitters and receivers. So instead of measuring the time or angle of received signals, the signal strength is measured. This negates many of the above mentioned deficiencies. A similar system has been employed by animal trackers with directional antennas.
			\cite{yongguang_chen_signal_2002}
	
		\subsubsection{Blue-tooth systems}
			Blue-tooth positioning works by using proximities, as opposed to angulation or lateration. As such, exact locations are not attainable using this technology. Instead, the system acts more as a geofence and works by determining which node a device is currently connected to in order to determine the location of the device.
		
			Apple has developed a protocol called iBeacon. This protocol makes use fo Blue-tooth proximity techniques in order to enable smart devices to perform actions when in close proximity to a particular iBeacon.
			\cite{_everything_????}
		
		\subsubsection{Choke point concepts}
			Choke point systems work by locating and indexing tagged objects in order to track them. The concept works by passing tagged objects though a choke point (or gate), the choke point will then have a sensor which detects the tagged object passing through the gate. Many choke point sensors work with passive radio-frequency identification (RFID) tags which merely report a tags presence, as opposed to reporting distances or signal strengths.
			\cite{reza_investigation_2008}
		
		\subsubsection{Grid concepts}
			Grid concepts employ a dense network of low-range receivers arranged in a predefined known pattern. A tagged object within the grid will be sensed by only a few nearby receivers. By determining which receivers and tag is sensed by, a rough approximation of the location of the tagged object can be made. The tag would typically be a radio frequency identification (RFID) chip.
		
		\subsubsection{Others}
			Various other radio based systems exist but will not be discussed further. These include ultra-wide band (UWB), infra-red (IR), visible light communication, as well as ultrasound.
	
	\section{Non-radio technologies}
		Non-radio technologies which can be used for indoor positioning will be discussed in the subsections below. These systems can provide increased accuracy at the expense of increased costs of equipment as well as installation costs.
	
	\subsection{Magnetic positioning}
		Magnetic positioning takes advantage of the way iron in buildings affects the Earth's magnetic field. The iron creates local variations in the Earth's magnetic field which can then be sensed by compasses to map indoor locations. These local variations are mapped which then allows devices equipped with a compass to navigate and determine their locations within the indoor environment.
		\cite{supreeth_sudhakaran_geospatial_2014}
		
		Indoor Atlas is a company which provides an indoor mapping service which makes use of these local variations of Earth's magnetic field. The company provides a mobile application which enables a user to map out a particular building, and allows users to navigate the created map.
	
	\subsection{Inertial measurements}
		Inertial measurement units (IMUs) can be carried by an object in order to track the objects path through space. IMUs measure acceleration and orientation along three orthogonal axis using accelerometers and gyroscopes. Position can be determined by double integration of the acceleration measurements - this is a form of dead reckoning.
		
		Dead reckoning is the process of calculating ones current location by using previously determined positions with estimated speed and orientation over some time. This yields relative position estimations. 
		
		Dead reckoning is subject to what is known as drift which is an accumulation of errors. Due to the susceptibility of IMUs to drift, they are often used in conjunction with other positioning systems in order to correct for this drift.

\newpage
\chapter{Method}
	There are two main requirements for this alternate indoor positioning method. The first requirement is to have a 3D model of an environment, the second requirement is a photograph of a scene within that environment. The premise is that the photograph can be matched against the 3D model in order to determine where the photograph was taken within that 3D model, thus locating the photographer. The focus of this paper is on the comparison of the photograph and the 3D model.
	
	The querying of the photograph against the model presents a problem. The problem is that the photograph and the 3D model are not intrinsically comparable. So how does one compare an image to a 3D model? A solution to this problem is to create renderings throughout the model and compare these renderings to the photograph. These renders are themselves images and so this permits the use of advanced and robust image recognition software to perform the matching.
	
	In the following sections, the processes of creating a 3D interior model, taking a photograph and comparing a photograph to renders of the 3D model will be discussed in detail.
	
	\section{Photograph}
		In this alternate indoor positioning system, a photograph will be taken by a person who wishes to know their location within an indoor environment, which will have an associated 3D model. The photograph will typically be taken from a mobile device such as a smart phone or tablet.
	
	\newpage
	\section{The 3D model}
		3D modelling can be described as the process of developing a mathematical representation of a three dimensional space using specialized computer software. The product of this process is called a 3D model. A 3D model can be visualized as a two dimensional image through a process called 3D rendering. It is this two dimensional image representation which is utilized in order to perform the comparison to a photograph in this alternate indoor positioning system. In the subsections to follow, the process of creating a 3D model and associated renderings will be discussed and detailed.
		
		\subsection{Creating a 3D model}
			3D models can be created automatically or manually. Examples of automatic techniques include structure from motion. Manual creation of 3D models involves construction by hand using advanced computer software such as Blender. 
			In order to accurately model an existing eprecedentnvironment, raw data such as measurements or point clouds is needed to serve as a basis for the model creation. The raw data serves as the initial building block and reference for the model.
		
			\subsubsection{Data acquisition}
				In order to create an accurate 3D model of a building, data is required in order to aid the modelling process and serve as a template or reference.
				This data can come in the form of building plans, measured distances and directions or point clouds etc.
				Creating a model from building plans or other measurements involves defining model features in terms of parameters.
				It is often the case that the measurements or plans do not accurately reflect the real-world environment in which case the created model may contain inaccuracies. It is for this reason that point clouds are preferable, as point clouds will more accurately represent the physical real-world environment.
				Creating a model from a point cloud involves creating vertices, lines and faces based on the positions of points  within the point cloud.
				
			\subsubsection{Data preprocessing}
				\paragraph{Cleaning laser scan point clouds}
					\subparagraph{Irrelevant points}
						During a scan, it often happens that a scanner captures points which are irrelevant. These irrelevant points may be points which are considered unneeded or erroneous in a particular scan. Typical examples of cases leading to irrelevant points are people walking through a scan, reflections of the scanners laser beam off of surfaces, or points from objects which are generally deemed irrelevant or unneeded.
						
						\begin{figure}[H]
							\centering
							\includegraphics[width=1\textwidth]{uncleaned_point_cloud}
							\caption{Raw point cloud containing irrelevant points}
							\label{fig:uncleaned_point_cloud}
						\end{figure}
						
						In order to treat these irrelevant points within a point cloud, a point cloud can be cleaned. Cleaning can be performed manually by selectively removing irrelevant points, or automatically be defining some criterion by which points which should be removed.
					
						\begin{figure}[H]
							\centering
							\includegraphics[width=1\textwidth]{cleaned_point_cloud}
							\caption{Cleaned point cloud containing with removed irrelevant points}
							\label{fig:cleaned_point_cloud}
						\end{figure}
						
					\subparagraph{Down sampling}
						Due to laser beam divergence or the scanners laser, points nearer to the scanner are packed more densely than those farther away. The density decreases radially from the scanners location.
						
						\begin{figure}[H]
							\centering
							\includegraphics[width=1\textwidth]{dense_point_cloud}
							\caption{Raw point cloud has a high point density near the location of the scanner}
							\label{fig:dense_point_cloud}
						\end{figure}
						
						It is often unnecessary to have the points packed at such a high density as the high density adds very little information to the overall point cloud. A process known as down sampling can be performed on a point cloud in order to make the point density more uniform. Down sampling works by iteratively removing points which fall within a specified distance of other points in a point cloud. The end result is that the point cloud's file size, and the computational effort required to visualize and manipulate the point cloud is greatly reduced.
						\cite{_selection_????}
						
						\begin{figure}[H]
							\centering
							\includegraphics[width=1\textwidth]{uniform_point_cloud}
							\caption{Down sampling yields a uniformly dense point cloud}
							\label{fig:uniform_point_cloud}
						\end{figure}
						
						Figure \ref{fig:dense_point_cloud} shows a point cloud with a high density near the scanners location. Most of the points near the scanner in the high density area are superfluous.
						
						Figure \ref{fig:uniform_point_cloud} shows a point cloud after a down sampling operation.
				
			\subsubsection{Constructing the model}
				Solid modelling is a consistent set of principles for mathematical and computer modelling of three-dimensional solids which forms the foundation of computer-aided model design. 
				\cite{vadim_shapiro_solid_2001}
				3D models can be created using various solid representation schemes. The common aim of these solid representation schemes is to organise modelled geometric and topological data in the form of consistent data structures. A model may be created using any number of combinations of solid representation schemes.
				
				\paragraph{Boundary representation (B-rep)}
					Boundary representation (B-rep) is a solid representation scheme which represents shapes using limits. In boundary representation, a solid is represented as a collection of connected surfaces. These surfaces are created using topological items: faces, edges and vertices. As a result of boundary representation modelling objects using limits, boundary representation lends itself to accurately modelling imperfections and inconsistencies.
					\cite{hongxin_zhang_introduction_2007}
				
				\paragraph{Constructive solid geometry (CSG)}
					Constructive solid geometry is a solid representation scheme which works by creating complex objects using boolean operators or primitive objects. Primitive objects are the simplest type of objects and are used to create more complex objects by performing operations such as unions, intersections and differences.
					\cite{foley_computer_1996}
				
		\subsection{Level of detail of the model}
			In terms of 3D modelling, level of detail refers to how thoroughly real-world objects have been modelled and how closely those modelled objects resemble their real-world counterparts.
			
			\begin{figure}[H]
				%TODO: Needs a source
				\centering
				\includegraphics[width=1\textwidth]{level_of_detail_example}
				\caption{Level of detail example}
			\end{figure}
			
			The figure above illustrates the concept of level of detail. A sphere has been modelled at various levels of detail, decreasing from left to right. Models with a low level of detail are quicker to create and less computationally intensive to manipulate, high level of detail models take longer to create but increase the quality of the model.
				
			\subsubsection{Lighting}
				Lighting is a very important component of any 3D model. Lighting can drastically alter the appearance of a model as well as its overall accuracy. When creating the model for this investigation, a great deal of attention was paid to lighting. Each consideration is discussed in detail below.
				
				\paragraph{Light source}
					When modelling light sources, there are a few considerations which need to be made, they are detailed in the paragraphs which follow.
					\subparagraph{Colour temperature}
						The colour temperature of a light source is the temperature of an ideal black-body radiator that radiates light of comparable hue to that of the light source. Colour temperatures are typically statued in the unit of absolute temperature, the Kelvin, with the symbol K.
						\begin{figure}[H]
							%TODO: Needs a source
							\centering
							\includegraphics[width=1\textwidth]{colour_temperature}
							\caption{Colour temperatures}
						\end{figure}
						The figure above shows how colour temperatures relate to real world conditions.
				
					\subparagraph{Position of lights}
						%TODO: Needs work, use better images
						In order to accurately replicate shadows and intensities, the lighting in a model needs to be strategically placed as well as calibrated.
						
						\begin{figure}[H]
							\centering
							\includegraphics[width=0.6\textwidth]{overhead_lights}
							\caption{Overhead florescent lights}
						\end{figure}
						\begin{figure}[H]
							\centering
							\includegraphics[width=0.6\textwidth]{lights_behind_door}
							\caption{Emmisive planes behind door replicate sunlight}
						\end{figure}
					
					\subparagraph{Light intensity}
						Luminous intensity is a measure of the wavelength-weighted power emitted by a light source in a particular direction. In order to accurately model the real world lighting intensities in a model, each light source needs to be adjusted individually and judged by visual inspection to best model the real-world light conditions.
						
			\subsubsection{Texture}
				Surface finish, also known as surface texture or surface topography, is the nature of a surface as defined by three characteristics of lay, surface roughness and waviness.
				\cite{e._paul_degarmo_materials_2003}
				Textures are applied to surfaces in 3D models in order to accurately represent real-world textures. Textures affect not only the visual appearance of a surface, but also the way light interacts with that textured surface.
					
			
		\subsection{Renderings of the model}
			Rendering can be described as the process of generating an image from a 2D or 3D model by means of computer programs. Renders can either be generated in real-time, or pre-rendered.
			
			Blender offers three built in render engines: Blender Render, Cycles Render, and Game Render.
			
			Renders of the 3D model will be created using the different renderer engines, and under various different render settings. The effect of altering the render settings will be a subject of investigation.
			
			\begin{figure}[H]
				%TODO: Needs a source
				\centering
				\includegraphics[width=0.9\textwidth]{gtl_render_1}
				\caption{GTL Render}
				\label{fig:gtl_render_1}
			\end{figure}
			
	\section{Comparison of the model to the photograph}
		This section deals with the comparison of the photograph to the render of the 3D model.
		
		The principle here is that there exist algorithms which, provided with a test image and query image, are able to detect the test image within the query image, yielding transformation parameters which relate the test image to the query image. These transformation parameters can be used together with photogrammetric principles to yield the location of the camera which took the photograph.
		
		\subsection{Performing the comparison}
			\subsubsection{Feature matching}
				\begin{figure}[H]
					%TODO: Needs a source
					\centering
					\includegraphics[width=0.9\textwidth]{feature_homography_example}
					\caption{Feature homography}
					\label{fig:feature_homogrophy}
				\end{figure}
				
				The figure above (Figure \ref{fig:feature_homogrophy}) illustrates the process of comparing a test image to a query image. The figure contains two images, one test image on the left side which is the cover of a rusk box. On the right is the query image. The query image is a more complex scene, which contains the test image within it. Here, the test image has been detected within the query image. An algorithm has drawn a green box around where it thinks the test image lies within the query image. This green box is defined by a transformation relating it to the test image. The process of performing the comparison will be discussed in detail in the sections to follow.
			
			\subsubsection{Key points}
				The first step involved in performing a comparison requires determining what are known as key points (or features) within an image. Key points represent specific patterns or features which are unique, can be easily tracked, and can be described and compared.
				
				To provide an example of what key points are, consider the image below:
				
				\begin{figure}[H]
					%TODO: Needs a source
					\centering
					\includegraphics[width=0.9\textwidth]{feature_building}
					\caption{Feature building}
					\label{fig:feature_building}
				\end{figure}
				
				The image is of a large building complex. At the top of the image there are six alphabetically labelled blocks. Each block contains a section of the entire image and can be thought of as a key point (feature). We now want to be able to find the exact location of these blocks within the original image. 
				Blocks A and B depict flat surfaces, they are spread over a large area and so it is difficult to find an exact location of these blocks within the image, that is that there are a number of locations where these blocks could fit, so they are not unique.
				Blocks C and D are slightly better in that they are the edges of the building, they could be from anywhere along the edge, but it is still impossible to pinpoint exactly where they are from.
				Blocks E and F are the corners of the building. The locations of these blocks can be exactly found and so these would be good key points as they are unique.
			
			\subsubsection{Finding key points}
				The process of finding key points in an image is called feature detection. On a high level, the process works by looking for regions within an image which have large variation when moved in all directions. This process will be discussed in more detail below.
				
				Consider the image below:
				
				\begin{figure}[H]
					%TODO: Needs a source
					\centering
					\includegraphics[width=0.9\textwidth]{feature_detection}
					\caption{Feature detection}
				\end{figure}
				
				In the figure above, the green rectangle represents an image and each red, blue and black rectangle represents a region for feature detection. When moving the blue rectangle vertically or horizontally by small amounts, there is no variation, the blue box still contains only green. This represents a poor feature/key point. 
				When moving the black rectangle horizontally, there is also no variation, however when moving it vertically there is (The ratio of white and green changes). When moving the red rectangle in any direction there is high variation, so this would be a good feature/key point. So corners are typically good features.
				
			\subsubsection{Describing key points}
				Once key points have been identified, they need to be described. The purpose of describing the key points is to enable the comparison of key points across different images. The idea is that a real-world point will have a similar key point description within all the images which capture the same real-world point.
				
			\subsubsection{Matching key points}
				The final step of the comparison process is that of matching key points. The concept here is that key points from separate images are individually compared against each other. If the similarity of the description of any two key points exceeds some threshold, the compared key points are considered a match which would ideally imply that they are the same real-world point.
		
		\subsection{Comparing}
			Now that concepts of feature detection and feature matching have been introduced, the application of the image comparison process to the objectives of this paper can be discussed.
			In the context of this proposed new indoor positioning technique, the test image would be a photograph that a user has supplied who wishes to know their location within an environment. The query image would be one of many renders of the 3D model of that same environment.
			
			The idea is that the test image could be compared to a number of renders from different locations within the environment. Once a quality comparison is made, the location of the photographer can be determined using photogrammetric techniques, or by using georeferenced renders, however this is outside of the scope of this paper, but will be further discussed in the future work chapter below.
			%TODO: Reference to Future Work chapter
	
	\section{Tools}
		\subsection{OpenCV}
			OpenCV (Open Source Computer Vision) is a library of programming functions mainly aimed at computer vision. OpenCV was originally developed by Intel in Russia. OpenCV offers a large range of features and functions, including an image processing module, GUI module, camera construction and 3D reconstruction module, a 2D features framework module as well as many others. The 2D features module (features2d) was used in this investigation to facilitate the comparison of the photograph to the renders. 

	\section{Test parameters}
	\label{test_parameters}
		%TODO: This entire sections needs some attention
		There are various actions which can be performed on the model, rendered image and photograph which may influence the accuracy of, and the number of matches obtained during a comparison. These actions and their result on the accuracy of the matches are to be a subject of investigation. These actions are discussed below.
		
		\subsection{Model adjustments}
			\subsubsection{Level of detail}
				The level of detail of the model can be adjusted.
				
			\subsubsection{Image draping}
				Image draping can be performed.
			
			\subsubsection{Lighting}
				Lighting in the model can be adjusted.
		
		\subsection{Renders}
			Render settings can be adjusted.
			
		\subsection{Image enhancements}
			A number of image enhancement processes can be performed on the photograph as well as on the rendered image. These processes will be discussed in subsections to follow.
			\subsubsection{Blurring}
				Gaussian blurring (or smoothing) is an image processing function which yields an image blurred by a Gaussian function. It is typically used to reduce noise and detail within an image. Gaussian blur acts as a low pass filter, attenuating high frequency signals.
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.6\textwidth]{gaussian_blur}
					\caption{Gaussian blur example (Source: wikipedia.org)}
					\label{fig:gaussian_blur}
				\end{figure}
			
			\subsubsection{Edge detection}
				Edge detection is a method of identifying points which are discontinues, or where the brightness changes sharply.
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{edge_detection}
					\caption{Edge detection example (Source: wikipedia.org)}
					\label{fig:edge_detection}
				\end{figure}

\chapter{Results}
	The Geomatics Teaching Laboratory (GTL) has been used as the subject for this investigation. The Geomatics Teaching Laboratory is located on the fifth floor of the Menzies building at UCT. The front door section of the Geomatics Teaching Laboratory was used as the basis of this investigation. The section under investigation is the south east corner of the room and was selected as there are a number of favourable features which are expected to maximize the chances of a high quality match. Some of these favourable features include the whiteboard, doors, skirting, air-conditioner vents as well as the double lipped corner with a conduate running along an edge.
	
	%TODO: Explain why GTL was selected
	
	\section{Photograph}
		The test photograph for this investigation is of the front door section of the Geomatics Teaching Laboratory (GTL). The scene consists of a air-conditioning duct on which there is a vent. On the left of the image is a section of a pin board, which contains a number of newspaper clippings. In the centre of the image is the corner of the room. The corner is relatively complex with a number of conduates running to the floor, with an electrical plug box. The blue doors are symmetrical with the exception of the bottom panels where the right hand door as a black ventilation panel. The other three panels are semi transparent glass panes which allow diffused light to enter the room.
		
		The photograph was taken using an iPhone 4S camera, which is actually an eight megapixel Sony Exmor R IMX 145.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=1\textwidth]{gtl_door_area}
			\caption{GTL Door area}
		\end{figure}
		
		%TODO: Add camera distortion parameters etc
		
	\section{3D model}
		The model was created in an open source software package called Blender.
		
		\subsection{Creating the 3D model}
			\subsubsection{Data acquisition}
				In order to accurately model the scene of interest, a laser scan of GTL was conducted. The laser scanner was placed roughly in the centre of the room and a single scan was taken which captured the entire room. The scan was carried out using a {{laser Scanner model}} at {{laser scanner settings, resolution etc}}. The laser scan yielded a high resolution point cloud which would serve as the basis for the 3D model construction.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\textwidth]{raw_point_cloud_1}
					\caption{Raw point cloud}
				\end{figure}
				
			\subsubsection{Data preprocessing}
				The point cloud in question was manually cleaned in a software program called CloudCompare. The purpose of cleaning the point cloud is to remove erroneous and superfluous data in order to increase the manageability of working with the point cloud.
	
				The first step cleaning the point cloud involved cutting out the sections which were not to be part of this investigation. This trivial process was performed in a software package called CloudCompare.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\textwidth]{trimmed_point_cloud_1}
					\caption{Trimmed point cloud}
				\end{figure}
				
				The second step in cleaning the point cloud involved removing points which existed outside of the rooms confines, such as points outside of the glass panels of the door.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\textwidth]{cleaned_point_cloud_1}
					\caption{Cleaned point cloud}
				\end{figure}
 
				The last step in the cleaning process involved making the point cloud uniform in density or resolution. This was done by determining the lowest resolution within the remaining section of the point cloud. This was found to be 7mm and was an acceptable resolution for use throughout the entire scene. So the entire point cloud was down sampled to a resolution of 7mm. This removed superfluous points and considerably reduced the file size of the point cloud.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=1\textwidth]{uniform_point_cloud_1}
					\caption{Uniform point cloud}
				\end{figure}
			
			\subsubsection{Model construction}
				The 3D model in this investigation was created using a solid modelling technique called boundary representation (B-rep). Boundary representation is a solid representation scheme. It is a method for representing shapes using limits or edges. In boundary representation, a solid is represented as a collection of connected surfaces. The surfaces themselves are created using topological items: faces, edges and vertices.
				\cite{hongxin_zhang_introduction_2007}
				Boundary representation was selected as the solid representation scheme as it is very well suited to modelling inconsistent or imperfect objects. For example, many objects in the scene appear to be perpendicular, but upon closer inspection it can be seen that very few corners are true, and very few edges are perfectly straight. Boundary representation lends itself to accurately modelling these imperfections which leads to a model which is overall more accurate, in that the model more accurately represents the real-world features.
				
				The accuracy of the model is an important consideration for this investigation. The model was constructed as accurately as possible in order to maximise the chances of obtaining good quality matches, and to serve as a point of further investigation (by investigating the effects of model quality/accuracy on the quality and number of matches).
				
				The software used to create and manipulate the 3D model was Blender. Blender is a free and open-source computer graphics software product. The cleaned point cloud was loaded into Blender and faces were attached to points in order to create the outline of the room, the outline consisted of the four surrounding walls, floor and ceiling. 
				
				{Insert image of simple model}
				
				With the extremities constructed, finer features and details were iteratively added to the model, this included the white boards on the walls, the skirting along the bottom of the walls, as well as conduates and electrical plug points, as well as the door section.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{simple_model_with_pc}
					\caption{Walls, floor and air-conditioning duct modelled from the point cloud}
					\label{fig:simple_model}
				\end{figure}
				
				In figure \ref{fig:simple_model} the walls and floor of the room have been modelled off of the point cloud. 
				
				Detail of the model was iteratively increased by modelling the finer details.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{model_with_increased_detail}
					\caption{Model with increased detail}
					\label{fig:more_complex_model}
				\end{figure}
				
				Figure \ref{fig:more_complex_model} shows the model with increased detail. The skirting board, conduate, door as well as white-boards have now been added. All these features have been modelled from the point cloud and so the model is an accurate representation of the actual room in terms of scale etc.
				
		\subsection{Level of detail of the 3D model}
			\subsubsection{Objects}
			%TODO: Refer to actual section. 'More on this later.'
				As many objects as possible have been modelled to the best possible detail. This is done in order to facilitate investigating the effects of the number of objects and the level of detail of these objects on the quality and number of matches in the comparison process. The investigation will cover multiple level of details in order to document the affect. More on this later.
				
			\subsubsection{Lighting}
				The GTL room is illuminated by florescent light panels, each containing three fluorescent tubes. The panels are rectangular with dimensions 0.5m by 1.5m. Additionally, light enters the room from the windows along the north wall, as well as through the glass panels within the door. So there are three sources of light illuminating the room.
			
				Much attention was given to the lighting within the model. This included accurately representing real-world lighting conditions within the model itself. The intensity, colour temperature and locations of light sources are all very important considerations when creating accurate 3D models.
				
				An important consideration for lighting with regards to this investigation is that lighting can vary. This includes the the variation of natural light over the diurnal cycle as well as variations due to weather conditions. Variations in artificial lighting is also important, such variations include lights being switched on or off, varying brightness etc.
				
				\paragraph{Overhead lights}
					Overhead lights consisted of panels of florescent lights uniformly spaced throughout the ceiling. The each panel contained three fluorescent tubes. Theses light panels were reconstructed in the model and mimicked using emmisive planes subtended within a rectangle.
					
					The overhead lights have been placed in their respective real-world positions. 
					
					{{Image of modelled florescent lights}}
				
				\paragraph{Natural lighting}
					In order to accurately model the natural light which penetrates though the glass panes of the door, emmisive planes were placed behind the glass panes of the door section. These emmisive planes were each calibrated in order to best represent real-world conditions of the photograph in terms of position, intensity and colour.
					
					{{Image of emmisive planes behind doors}}
			\subsubsection{Texture}
				Texture affects how an object interacts with light. Textures can be rough or smooth and can be arranged randomly or uniformly. Textures can reflect light specularly (mirror-like) or diffusely (randomly).
				
				%TODO: Explain texture use in the model
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{light_reflection}
					\caption{Specular and diffuse light reflection (Source:vacuumcoating.indo)}
					\label{fig:light_reflection}
				\end{figure}
				The objects in the model have all been assigned textures in order to best reflect their real-world characteristics. The texture of objects will be another subject of investigation in this report. More on this later.
				
				%TODO: Needs work
				Particular attention has been paid to accurately representing surface characteristics of as many features as possible in the model.
				The panels in the door are set to be smooth and transparent, replicating the real-world glass panes. 
				The texture of the walls has been adjusted.
				The texture of the floor has been roughened to replicate the carpet.
				The whiteboards are smooth and glossy.
				The gray pin board has a rough texture.
				
			\subsubsection{Image draping}
				It is further possible to drape images of objects within the 3D model. Blender offers a feature known as UV Unwrap which enables an image to be wrapped around a modelled object. The texture of the object then becomes the image.
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{blender_uv_unwrap_1}
					\caption{3D model is in the left pane, the right pane contains the image which will be wrapped onto the model (Source: ytimg.com)}
					\label{fig:blender_uv_unwrap_1}
				\end{figure}
				
				\begin{figure}[H]
					\centering
					\includegraphics[width=0.9\textwidth]{blender_uv_unwrap_2}
					\caption{The result of wrapping the model with the image (Source: vimeocdn.com)}
					\label{fig:blender_uv_unwrap_2}
				\end{figure}
				
				Image draping is another subject of investigation.
				
		\subsection{Renders}
			Renders for this investigation where created using Blender's Cycles Renderer as it provided superior render quality when compared to the standard Blender Renderer.
			
			{{Image of different renderers}}
			
			Two quality settings where adjusted when creating renders.
			%TODO: Describe the quality settings, add images.

	\section{Comparing the photograph to the model}
		In order to perform the comparison between the photograph and renders of the model, a testing framework has been created.
		The OpenCV features2d module offers various feature detection algorithms (SIFT, SURF, ORB) as well as feature matching algorithms (FLANN, BruteForce). Another OpenCV module used in this investigation was the image processing module which offers a number of image enhancement/manipulation functions which have been made use of in this investigation.
		
		\subsection{Testing framework}
			%TODO: Needs work
			In order to facilitate the investigative process, a framework was created using the programming language Python. This framework encapsulated all the various features and functions of OpenCV pertaining to this paper.
			
			The framework has a TestCase class which encapsulates a test image, query image, a feature detection method and a feature matching method. Additionally, a TestCase encapsulates any number of image processing functions and parameters which are programmability applied to a test and or query image.
			
			{{Insert image of creating a TestCase with iPython}}
			
		\subsection{Comparison results}
			The comparison process was run a number of times with varying parameters adjusted in order to investigate and document the effects of the parameters on the quality of each comparison. The parameters which where adjusted have been discussed above in section \ref{test_parameters}. The results of these test parameters will be presented below.
			
			\subsubsection{Photograph adjustments}
				\paragraph{Blurring}
				\paragraph{Edge detection}
				
			\subsubsection{Model adjustments}
				\paragraph{Level of detail}
				\paragraph{Image draping}
				\paragraph{Lighting}
				\paragraph{Renders}
				
			\subsubsection{Framework adjustments}
				\paragraph{Detectors}
				\paragraph{Matchers}

\chapter{Discussions}
	\section{Comparison quality}
		The quality of a comparison is determined by two factors, the number of matches, and the quality of the matches. The number of matches refers to the number of matched feature/key point pairs found in the comparison process. The quality of a match refers to the accuracy of a matched key point pair, basically the degree to which a matched key points represent the same real world point. So a match would be of high quality if the key points of the match precisely represent the same real-world point.
		Determining and describing the quality of matches poses a problem, and that is that it is difficult to programmatically quantify the quality of a match. In order to solve this problem, matches are visually inspected in order to determine their quality.

\chapter{Conclusions and future work}

\newpage
\printbibliography

\end{document}